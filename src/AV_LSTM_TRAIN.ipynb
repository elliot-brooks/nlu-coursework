{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elliot-brooks/nlu-coursework/blob/main/src/AV_LSTM_TRAIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX1sFbdQgv_W"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers\n",
        "!pip install -U accelerate"
      ],
      "metadata": {
        "id": "rnwfUvsFMgc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwaoFym8guws"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
        "import nltk\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd7312r4g-Fs"
      },
      "source": [
        "# Load training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvOX2Y9chCJ6"
      },
      "outputs": [],
      "source": [
        "training_corpus = pd.read_csv(\"train.csv\", encoding='utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h7JHjvThCjX"
      },
      "source": [
        "# Pre-process training data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Case fold to lower-case\n",
        "def preprocess(string):\n",
        "  output = str(string).lower()\n",
        "  separated_string = re.sub(r'([^\\w\\s])', r' \\1 ', str(string))\n",
        "  return output\n",
        "\n",
        "# Prepare data for Distilled Bert by concatenating pairs with [SEP] token\n",
        "def prepare_data(data) :\n",
        "  data[\"text_1\"] = data[\"text_1\"].apply(lambda x: preprocess(x))\n",
        "  data[\"text_2\"] = data[\"text_2\"].apply(lambda x: preprocess(x))\n",
        "  concat_pairs = []\n",
        "  for index, row in data.iterrows():\n",
        "      concatenated_pair = row[\"text_1\"] + \" [SEP] \" + row[\"text_2\"]\n",
        "      concat_pairs.append(concatenated_pair)\n",
        "  return concat_pairs\n",
        "\n",
        "concat_data = prepare_data(training_corpus)"
      ],
      "metadata": {
        "id": "upW_8gLfMIlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create BERT embeddings"
      ],
      "metadata": {
        "id": "IFUsxP4PL9VN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokeniser = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "bert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "yUN7QY7jL8zI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LENGTH = 256\n",
        "BATCH_SIZE = 32\n",
        "def create_bert_embeddings_batch(texts, tokeniser, model, batch_size, seq_length) :\n",
        "  embeddings = []\n",
        "  for i in range(0, len(texts), batch_size) :\n",
        "    batch = texts[i:i + batch_size]\n",
        "    inputs = tokeniser.batch_encode_plus(batch, padding='max_length', truncation=True, return_tensors='tf', max_length=seq_length, add_special_tokens=True)\n",
        "\n",
        "    # Create embeddings\n",
        "    outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "\n",
        "    last_hidden_state_CLS = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "    embeddings.append(last_hidden_state_CLS)\n",
        "  return embeddings\n",
        "\n",
        "bert_embeddings = create_bert_embeddings_batch(concat_data, tokeniser, bert_model, BATCH_SIZE, SEQ_LENGTH)\n",
        "train_labels = np.array(training_corpus['label'])"
      ],
      "metadata": {
        "id": "Kh38fXs3N_tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define classification model"
      ],
      "metadata": {
        "id": "5i8SoOuUMuAM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRj2twAchU33",
        "outputId": "753eb4a6-8213-49eb-d3fc-394deb0f6813",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)        [(None, 150)]                0         []                            \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)        [(None, 150)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)     (None, 150, 100)             1062880   ['input_5[0][0]',             \n",
            "                                                          0          'input_6[0][0]']             \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)               (None, 128)                  117248    ['embedding_2[0][0]',         \n",
            "                                                                     'embedding_2[1][0]']         \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, 256)                  0         ['lstm_2[0][0]',              \n",
            " )                                                                   'lstm_2[1][0]']              \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 1)                    257       ['concatenate_2[0][0]']       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10746305 (40.99 MB)\n",
            "Trainable params: 10746305 (40.99 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "LSTM_UNITS = 128\n",
        "DROPOUT_RATE = 0.2\n",
        "model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(768, 1)),\n",
        "        tf.keras.layers.LSTM(LSTM_UNITS, dropout=DROPOUT_RATE),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "# Summarise Model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyEe9fxZhKad"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xylsLNIhVLd",
        "outputId": "1b1b584d-7969-42ba-dab8-0f24c5b5a5c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "938/938 [==============================] - 76s 78ms/step - loss: 0.6897 - accuracy: 0.5226\n",
            "Epoch 2/10\n",
            "938/938 [==============================] - 27s 29ms/step - loss: 0.5740 - accuracy: 0.7064\n",
            "Epoch 3/10\n",
            "938/938 [==============================] - 22s 24ms/step - loss: 0.2840 - accuracy: 0.8779\n",
            "Epoch 4/10\n",
            "938/938 [==============================] - 19s 20ms/step - loss: 0.1159 - accuracy: 0.9544\n",
            "Epoch 5/10\n",
            "938/938 [==============================] - 20s 21ms/step - loss: 0.0558 - accuracy: 0.9789\n",
            "Epoch 6/10\n",
            "938/938 [==============================] - 18s 19ms/step - loss: 0.0295 - accuracy: 0.9894\n",
            "Epoch 7/10\n",
            "938/938 [==============================] - 23s 24ms/step - loss: 0.0201 - accuracy: 0.9937\n",
            "Epoch 8/10\n",
            "938/938 [==============================] - 18s 19ms/step - loss: 0.0153 - accuracy: 0.9949\n",
            "Epoch 9/10\n",
            "938/938 [==============================] - 19s 20ms/step - loss: 0.0101 - accuracy: 0.9968\n",
            "Epoch 10/10\n",
            "938/938 [==============================] - 17s 19ms/step - loss: 0.0111 - accuracy: 0.9963\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fdd1c6c1510>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "train_inputs = np.concatenate(bert_embeddings, axis=0).reshape(-1, 768, 1)\n",
        "print(train_inputs.shape)\n",
        "model.fit(train_inputs, train_labels, epochs=100, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHHs6mBehNBR"
      },
      "source": [
        "# Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_EptFJXhVyf"
      },
      "outputs": [],
      "source": [
        "model.save(\"AV_LSTM_MODEL\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip model file\n",
        "!zip -r /content/LSTM_MODEL.zip /content/AV_LSTM_MODEL"
      ],
      "metadata": {
        "id": "e3lHpKM7FkSg",
        "outputId": "a409ab94-1670-49ed-a2bb-250bc64a559b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/AV_LSTM_MODEL/ (stored 0%)\n",
            "updating: content/AV_LSTM_MODEL/keras_metadata.pb (deflated 88%)\n",
            "updating: content/AV_LSTM_MODEL/variables/ (stored 0%)\n",
            "updating: content/AV_LSTM_MODEL/variables/variables.index (deflated 59%)\n",
            "updating: content/AV_LSTM_MODEL/variables/variables.data-00000-of-00001 (deflated 10%)\n",
            "updating: content/AV_LSTM_MODEL/fingerprint.pb (stored 0%)\n",
            "updating: content/AV_LSTM_MODEL/assets/ (stored 0%)\n",
            "updating: content/AV_LSTM_MODEL/saved_model.pb (deflated 90%)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}