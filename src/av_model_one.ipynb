{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elliot-brooks/nlu-coursework/blob/main/src/av_model_one.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX1sFbdQgv_W"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "gwaoFym8guws"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "FIRST_PAIR = 0\n",
        "SECOND_PAIR = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd7312r4g-Fs"
      },
      "source": [
        "# Load training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "pvOX2Y9chCJ6"
      },
      "outputs": [],
      "source": [
        "training_corpus = pd.read_csv(\"train.csv\", encoding='utf-8')\n",
        "# pairwise_labels = np.array(training_corpus['label'])\n",
        "# pairwise_data = np.array(training_corpus[['text_1', 'text_2']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h7JHjvThCjX"
      },
      "source": [
        "# Pre-process training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "tQCBFdCvhUit"
      },
      "outputs": [],
      "source": [
        "def preprocess(string):\n",
        "  output = str(string).lower()\n",
        "  separated_string = re.sub(r'([^\\w\\s])', r' \\1 ', str(string))\n",
        "  return output\n",
        "\n",
        "training_corpus[\"text_1\"] = training_corpus[\"text_1\"].apply(lambda x: preprocess(x))\n",
        "training_corpus[\"text_2\"] = training_corpus[\"text_2\"].apply(lambda x: preprocess(x))\n",
        "pairwise_labels = np.array(training_corpus['label'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenise(data, column_1, column_2, max_sequence_length) :\n",
        "  first_pairs = training_corpus[column_1].tolist()\n",
        "  second_pairs = training_corpus[column_2].tolist()\n",
        "\n",
        "  # init tokeniser\n",
        "  tk = Tokenizer(oov_token='UNK', lower=True)\n",
        "  tk.fit_on_texts(first_pairs + second_pairs)\n",
        "\n",
        "  # tokenise texts\n",
        "  tokenised_first_pairs = tk.texts_to_sequences(first_pairs)\n",
        "  tokenised_second_pairs = tk.texts_to_sequences(second_pairs)\n",
        "\n",
        "  # pad sequences\n",
        "  tokenised_first_pairs = pad_sequences(tokenised_first_pairs, maxlen=max_sequence_length, padding='pre')\n",
        "  tokenised_second_pairs = pad_sequences(tokenised_second_pairs, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "  # return vocabulary\n",
        "  vocab = tk.word_index\n",
        "\n",
        "  return tokenised_first_pairs, tokenised_second_pairs, vocab\n",
        "\n",
        "\n",
        "sequences_1, sequences_2, vocab = tokenise(training_corpus, \"text_1\", \"text_2\", 150)"
      ],
      "metadata": {
        "id": "Kh38fXs3N_tk"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequences_1[0])\n",
        "print(sequences_2[0])"
      ],
      "metadata": {
        "id": "aECEykqSXKj7",
        "outputId": "e9334916-4cb7-4982-db93-b2e5ffb0a433",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[    0     0     0     0     0     0     0     0     0     0     0  1623\n",
            "   755  2397   385   133     3    30  9015     3   184   665     6    30\n",
            "  2829   592  1425  7812    67    32  4378    36   216  1218  2366   432\n",
            " 13494     5    30   670   419  6011 42618    32    95   302   747    18\n",
            "     4   928   215   216   787   746    11   153    23   187     3    73\n",
            "    18    97     7  1474    16     8     4  3761   133     8  9281    14\n",
            "    21   259     6     2  3096    46  3039     5     7    85   542    14\n",
            "    85    62  2162   670  2421    84    14   182    14    57   266    23\n",
            "   120     6     4   127     3   123    18    14    95     2    46  1891\n",
            "    37  2350     7  2549  2506  3267   554    14   220   399   197   315\n",
            "     7  2506  3267   220    16    46     9  2011     5  1247    63    34\n",
            "     6   196  1116   199   568     6   849   329    45   985  5654     5\n",
            " 16969  4728  1524   534     2   751]\n",
            "[    0     0     0     0     0     0     0     0     0     0     0  1245\n",
            "  7416  2159 20367  4284  9095     9  9997    29     2  1245    22  3012\n",
            "   817 49983  1349  8171 20367  1634     2  4254    19     4  1492     3\n",
            "   184     4   227  3793  2217    15   539    24 49983  2143    11     5\n",
            "   564    14    15   539    60  1220     9     4  1404     5  2435   492\n",
            "   333 20367     5 49983    18     4   227  3016     5   773    84   161\n",
            "  1807    59    63    84   681     5  8407    24     2   759    48   799\n",
            "    62     6     2  3016     5   773    27  2050     7    80    45 19096\n",
            " 87846   301    21    28   430     2    74   123     5 12891    95     2\n",
            "  1285   329   438    22 58112     5 28609   645    27   974  4290   745\n",
            "    57    67     3   466     2   710    15     2   566     3   319    16\n",
            "    46  5683     3  8171     5  9095  9095     9    57   286     7   192\n",
            "   248    32    21   120     6    36]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo3REjQwhGU_"
      },
      "source": [
        "# Define Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "iRj2twAchU33"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyEe9fxZhKad"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "4xylsLNIhVLd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHHs6mBehNBR"
      },
      "source": [
        "# Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "r_EptFJXhVyf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}