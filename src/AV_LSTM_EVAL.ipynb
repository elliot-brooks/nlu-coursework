{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "0yVnYDbyH6d8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import re"
      ],
      "metadata": {
        "id": "jmdCxqr5H8Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXj36JOLHs4c"
      },
      "source": [
        "# Load dev data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "DOef0VYcHs4d"
      },
      "outputs": [],
      "source": [
        "dev_corpus = pd.read_csv(\"dev.csv\", encoding='utf-8')\n",
        "dev_labels = np.array(dev_corpus['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zONqpCRSHs4d"
      },
      "source": [
        "# Load Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "4_52w5GgHs4d"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile('LSTM_MODEL.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('LSTM_MODEL')\n",
        "\n",
        "LSTM_MODEL = tf.keras.models.load_model(\"LSTM_MODEL/content/AV_LSTM_MODEL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare data"
      ],
      "metadata": {
        "id": "2tUanlOCL8k0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(string):\n",
        "  output = str(string).lower()\n",
        "  separated_string = re.sub(r'([^\\w\\s])', r' \\1 ', str(string))\n",
        "  return output\n",
        "\n",
        "def tokenise(data, column_1, column_2, max_sequence_length) :\n",
        "  first_pairs = data[column_1].tolist()\n",
        "  second_pairs = data[column_2].tolist()\n",
        "\n",
        "  # init tokeniser\n",
        "  tk = Tokenizer(oov_token='UNK', lower=True)\n",
        "  tk.fit_on_texts(first_pairs + second_pairs)\n",
        "\n",
        "  # tokenise texts\n",
        "  tokenised_first_pairs = tk.texts_to_sequences(first_pairs)\n",
        "  tokenised_second_pairs = tk.texts_to_sequences(second_pairs)\n",
        "\n",
        "  # pad sequences\n",
        "  tokenised_first_pairs = pad_sequences(tokenised_first_pairs, maxlen=max_sequence_length, padding='pre')\n",
        "  tokenised_second_pairs = pad_sequences(tokenised_second_pairs, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "  # return vocabulary\n",
        "  vocab = tk.word_index\n",
        "\n",
        "  return tokenised_first_pairs, tokenised_second_pairs, vocab\n",
        "\n",
        "def combine_pairwise_data(sequence_1, sequence_2) :\n",
        "  tuple_list = []\n",
        "  for i in range(len(sequence_1)) :\n",
        "    tuple_list.append((sequence_1[i], sequence_2[i]))\n",
        "  return np.array(tuple_list)\n",
        "\n",
        "def prepare_test_data(dev_data) :\n",
        "  dev_data[\"text_1\"] = dev_data[\"text_1\"].apply(lambda x: preprocess(x))\n",
        "  dev_data[\"text_2\"] = dev_data[\"text_2\"].apply(lambda x: preprocess(x))\n",
        "  SEQUENCE_SIZE = 150\n",
        "  sequences_1, sequences_2, vocab = tokenise(dev_data, \"text_1\", \"text_2\", SEQUENCE_SIZE)\n",
        "  return [sequences_1, sequences_2]\n",
        "\n",
        "input_data = prepare_test_data(dev_corpus)\n",
        "\n"
      ],
      "metadata": {
        "id": "QURjc6gkL-Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yeq4qfLHs4d"
      },
      "source": [
        "# Test Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "egxEx5yuHs4d",
        "outputId": "13911b7c-cf87-4d3f-abba-5a52b6dd615f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188/188 [==============================] - 26s 134ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions = LSTM_MODEL.predict(input_data)\n",
        "binary_predictions = (predictions >= 0.5).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save predictions"
      ],
      "metadata": {
        "id": "lfb_PHKHI3f9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_DF = pd.DataFrame(binary_predictions, columns=['prediction'])\n",
        "predictions_DF.to_csv('Group_26_B.csv', index=False)"
      ],
      "metadata": {
        "id": "2OsQaI6BI5fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cY6XJjgHs4d"
      },
      "source": [
        "# Generate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "se_DoTx1Hs4d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}