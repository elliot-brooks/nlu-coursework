{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP/9KOFIa+AmMg5SfTaHtUN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elliot-brooks/nlu-coursework/blob/main/AV_SVM_EVAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "hcDskpPG05-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers\n",
        "!pip install -U accelerate"
      ],
      "metadata": {
        "id": "MLFXdg1v1rnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuTUoXT903d6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
        "import torch\n",
        "import re\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dev data set"
      ],
      "metadata": {
        "id": "-0KGlYea1CN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_corpus = pd.read_csv(\"dev.csv\", encoding='utf-8')\n",
        "dev_labels = np.array(dev_corpus['label'])\n",
        "tokeniser = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "bert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "TSissvl51DnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model"
      ],
      "metadata": {
        "id": "rUEeKjbc1Fii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile('SVM_MODEL.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('SVM_MODEL')\n",
        "\n",
        "SVM_MODEL = tf.keras.models.load_model(\"SVM_MODEL/content/AV_SVM_MODEL\")"
      ],
      "metadata": {
        "id": "2dC6ZAru1Gsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare data"
      ],
      "metadata": {
        "id": "PAujHM3r1IJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(string):\n",
        "  output = str(string).lower()\n",
        "  separated_string = re.sub(r'([^\\w\\s])', r' \\1 ', str(string))\n",
        "  return output\n",
        "\n",
        "# Prepare data for Distilled Bert\n",
        "def prepare_data(data) :\n",
        "  data[\"text_1\"] = data[\"text_1\"].apply(lambda x: preprocess(x))\n",
        "  data[\"text_2\"] = data[\"text_2\"].apply(lambda x: preprocess(x))\n",
        "  concat_pairs = []\n",
        "  for index, row in data.iterrows():\n",
        "      concatenated_pair = row[\"text_1\"] + \" [SEP] \" + row[\"text_2\"]\n",
        "      concat_pairs.append(concatenated_pair)\n",
        "  return concat_pairs\n",
        "\n",
        "concat_data = prepare_data(dev_corpus)\n",
        "\n",
        "\n",
        "SEQ_LENGTH = 128\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def create_bert_embeddings_batch(texts, tokeniser, model, batch_size, seq_length) :\n",
        "  embeddings = []\n",
        "  for i in range(0, len(texts), batch_size) :\n",
        "    batch = texts[i:i + batch_size]\n",
        "    inputs = tokeniser.batch_encode_plus(batch, padding='max_length', truncation=True, return_tensors='tf', max_length=seq_length)\n",
        "\n",
        "    # Create embeddings\n",
        "    outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "\n",
        "    last_hidden_state_CLS = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "    embeddings.append(last_hidden_state_CLS)\n",
        "  return embeddings\n",
        "\n",
        "bert_embeddings = create_bert_embeddings_batch(concat_data, tokeniser, bert_model, BATCH_SIZE, SEQ_LENGTH)"
      ],
      "metadata": {
        "id": "i-2KKzWn1JmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Models"
      ],
      "metadata": {
        "id": "TXRVLamT1SGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = SVM_MODEL.predict(input_data)\n",
        "binary_predictions = (predictions >= 0.5).astype(int)"
      ],
      "metadata": {
        "id": "w-Rye-b11ToK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save predictions"
      ],
      "metadata": {
        "id": "tripjIs41WQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_DF = pd.DataFrame(binary_predictions, columns=['prediction'])\n",
        "predictions_DF.to_csv('dev_predictions.csv', index=False)"
      ],
      "metadata": {
        "id": "ONxye8xI1X9H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}